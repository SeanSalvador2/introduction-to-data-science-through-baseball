{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overview:** In the [previous notebook](../how-to-get-baseball-data/extracting_baseball_data.ipynb) we learned how to extract baseball data from multiple sources using Python. Now we put that data to work. This notebook introduces **unsupervised learning** — specifically **clustering** — and applies the **K-Means** algorithm to real 2024 baseball data. We will cluster individual pitches to rediscover pitch types, group pitchers into archetypes based on their arsenals, and identify team playing styles across the league.\n",
    "\n",
    "**Next Steps:** In the follow-up notebook (`advanced_clustering.ipynb`) we will explore additional clustering algorithms (DBSCAN, Hierarchical, Gaussian Mixture Models) and advanced evaluation techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Prerequisites & Setup](#1-prerequisites--setup)\n",
    "2. [Supervised vs. Unsupervised Learning](#2-supervised-vs-unsupervised)\n",
    "3. [Types of Unsupervised Learning](#3-types-of-unsupervised)\n",
    "4. [What Is Clustering?](#4-what-is-clustering)\n",
    "5. [The K-Means Algorithm](#5-k-means-algorithm)\n",
    "6. [Toy Example: K-Means on Synthetic Data](#6-toy-example)\n",
    "7. [Evaluating Clusters: Elbow Method & Silhouette Score](#7-evaluating-clusters)\n",
    "8. [From Toy Data to Real Data: Feature Scaling](#8-toy-to-real)\n",
    "9. [Baseball Example 1: Pitch Classification](#9-pitch-classification)\n",
    "10. [Baseball Example 2: Pitcher Archetypes](#10-pitcher-archetypes)\n",
    "11. [Baseball Example 3: Team Playing Styles](#11-team-playing-styles)\n",
    "12. [Summary & Next Steps](#12-summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prerequisites & Setup <a id=\"1-prerequisites--setup\"></a>\n",
    "\n",
    "We will build on the data extraction skills from the previous notebook and add machine learning and visualization libraries.\n",
    "\n",
    "**Packages we'll use:**\n",
    "\n",
    "| Package | Purpose |\n",
    "|---------|----------|\n",
    "| `pybaseball` | Statcast pitch data, FanGraphs stats |\n",
    "| `pandas` | Data manipulation |\n",
    "| `numpy` | Numerical operations |\n",
    "| `scikit-learn` | K-Means clustering, StandardScaler, evaluation metrics |\n",
    "| `matplotlib` | Static visualizations |\n",
    "| `seaborn` | Statistical visualizations |\n",
    "| `plotly` | Interactive visualizations (hover labels) |\n",
    "\n",
    "**Requirements:**\n",
    "- Python 3.8+\n",
    "- An internet connection (we'll be pulling fresh data via pybaseball)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install pybaseball pandas numpy scikit-learn matplotlib seaborn plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base imports -- we'll import sklearn and plotly modules inline\n",
    "# in each section so you can see exactly where they come from\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Enable pybaseball caching so re-runs are instant\n",
    "from pybaseball import cache\n",
    "cache.enable()\n",
    "\n",
    "print(\"All base imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Supervised vs. Unsupervised Learning <a id=\"2-supervised-vs-unsupervised\"></a>\n",
    "\n",
    "Machine learning is typically split into two broad categories based on whether or not you have **labeled data** — that is, data where the \"right answer\" is already known.\n",
    "\n",
    "| | Supervised Learning | Unsupervised Learning |\n",
    "|---|---|---|\n",
    "| **Goal** | Predict a known label or outcome | Discover hidden structure in data |\n",
    "| **Data** | Labeled — features + target variable | Unlabeled — features only |\n",
    "| **Baseball example** | Predict if a pitch is a strike (label: ball/strike) | Group pitches by physical characteristics without knowing the pitch type |\n",
    "| **Common algorithms** | Linear Regression, Random Forest, Neural Networks | K-Means, PCA, DBSCAN |\n",
    "\n",
    "In supervised learning, you train a model on examples where the answer is provided, and then use that model to predict answers for new data. In unsupervised learning, there is **no answer key** — the algorithm's job is to find patterns and groupings on its own.\n",
    "\n",
    "This notebook focuses entirely on unsupervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Types of Unsupervised Learning <a id=\"3-types-of-unsupervised\"></a>\n",
    "\n",
    "Unsupervised learning encompasses several different tasks:\n",
    "\n",
    "- **Clustering**: Group similar items together. *Example: Group pitchers with similar arsenals into natural archetypes.*\n",
    "- **Dimensionality Reduction**: Compress many features into fewer, more informative ones. *Example: Summarize 300+ Statcast columns into 2–3 components that capture most of the variation.* (Covered in Part 2 of this series.)\n",
    "- **Anomaly Detection**: Identify unusual observations that don't fit the normal pattern. *Example: Flag a game where a pitcher's velocity drops 5 mph below his season average.*\n",
    "- **Association Rules**: Discover which items or events tend to co-occur. *Example: Pitchers who throw sliders frequently tend to also have higher strikeout rates.*\n",
    "\n",
    "This notebook focuses entirely on **clustering**. Dimensionality reduction is covered in Part 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. What Is Clustering? <a id=\"4-what-is-clustering\"></a>\n",
    "\n",
    "**Clustering** is the task of partitioning data into groups (called **clusters**) such that items within a group are more similar to each other than to items in other groups.\n",
    "\n",
    "**Intuition:** Imagine you have data on 1,000 pitches — just the speed and spin rate, with no labels telling you the pitch type. If you plotted speed vs. spin rate, you would probably see natural clumps: a group of fast, high-spin pitches (fastballs), a group of slower pitches with moderate spin (changeups), and a group with heavy break (curveballs). Clustering algorithms find these clumps automatically.\n",
    "\n",
    "**When is clustering useful?**\n",
    "- **Exploratory analysis**: What natural groupings exist in the data?\n",
    "- **Player/team profiling**: Are there distinct \"types\" of hitters, pitchers, or teams?\n",
    "- **Feature engineering**: Cluster assignments can become features for downstream supervised models\n",
    "- **Segmentation**: Group similar items for tailored strategies (e.g., pitch sequencing against different batter types)\n",
    "\n",
    "**Key property:** Clustering is unsupervised — you typically do **not** know the \"right\" number of groups or what those groups should be. The algorithm discovers the structure, and you bring domain knowledge to interpret it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. The K-Means Algorithm <a id=\"5-k-means-algorithm\"></a>\n",
    "\n",
    "**K-Means** is the most widely used clustering algorithm. It is simple, fast, and often the first tool to reach for when exploring your data.\n",
    "\n",
    "### How K-Means Works (Step by Step)\n",
    "\n",
    "1. **Choose K** — the number of clusters you want to find (you must decide this upfront)\n",
    "2. **Initialize** — randomly place K points in the feature space. These are the initial **centroids** (cluster centers)\n",
    "3. **Assign** — for each data point, compute the distance to every centroid and assign it to the **nearest** one\n",
    "4. **Update** — move each centroid to the **mean** of all points currently assigned to it\n",
    "5. **Repeat** steps 3–4 until the centroids stop moving (convergence)\n",
    "\n",
    "The algorithm minimizes the **within-cluster sum of squares** (WCSS) — the total distance between each point and its assigned centroid.\n",
    "\n",
    "### Key Properties\n",
    "\n",
    "| Property | Detail |\n",
    "|----------|--------|\n",
    "| Requires K upfront | You must specify the number of clusters before running |\n",
    "| Uses Euclidean distance | Features must be on similar scales (we'll cover scaling soon) |\n",
    "| May find local optima | Different random initializations can give different results — scikit-learn's `n_init` parameter runs the algorithm multiple times and keeps the best |\n",
    "| Fast | Time complexity is O(n × K × d × iterations), making it practical for large datasets |\n",
    "| Assumes round clusters | Works best when clusters are roughly spherical and similar in size |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing K-Means Step by Step\n",
    "\n",
    "Let's watch the K-Means algorithm in action on a small dataset. We'll manually run through 4 iterations to see how centroids move and cluster assignments change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Generate a small dataset to visualize the algorithm\n",
    "X_demo, _ = make_blobs(n_samples=50, centers=3, cluster_std=1.2, random_state=42)\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 4))\n",
    "\n",
    "for i, max_iter in enumerate([1, 2, 3, 10]):\n",
    "    # Run K-Means with a limited number of iterations\n",
    "    km = KMeans(n_clusters=3, init='random', n_init=1, max_iter=max_iter, random_state=42)\n",
    "    labels = km.fit_predict(X_demo)\n",
    "    centroids = km.cluster_centers_\n",
    "\n",
    "    ax = axes[i]\n",
    "    ax.scatter(X_demo[:, 0], X_demo[:, 1], c=labels, cmap='viridis', s=30, alpha=0.7)\n",
    "    ax.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='X', s=200, edgecolors='black', linewidths=1.5)\n",
    "    ax.set_title(f'Iteration {max_iter}' if max_iter < 10 else 'Converged')\n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n",
    "\n",
    "plt.suptitle('K-Means Algorithm: Step by Step', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Red X markers = centroids. Watch how they move toward the center of each cluster.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Toy Example: K-Means on Synthetic Data <a id=\"6-toy-example\"></a>\n",
    "\n",
    "Before working with real baseball data, let's practice on synthetic data where we **know** the true clusters. This lets us verify that K-Means is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Create 3 clusters of 2D data\n",
    "X, y_true = make_blobs(n_samples=300, centers=3, cluster_std=0.8, random_state=42)\n",
    "\n",
    "# Plot the raw data -- no labels, just points\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], s=30, alpha=0.7, color='gray')\n",
    "plt.title('Synthetic Data — Can You See the Clusters?')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Dataset: {X.shape[0]} points, {X.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Apply K-Means with K=3\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "kmeans.fit(X)\n",
    "\n",
    "labels = kmeans.labels_         # Cluster assignment for each point (0, 1, or 2)\n",
    "centroids = kmeans.cluster_centers_  # Coordinates of the 3 centroids\n",
    "\n",
    "print(f\"Cluster assignments: {np.unique(labels)}\")\n",
    "print(f\"Points per cluster: {np.bincount(labels)}\")\n",
    "print(f\"Inertia (WCSS): {kmeans.inertia_:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side: K-Means clusters vs. ground truth\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: K-Means result\n",
    "axes[0].scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=30, alpha=0.7)\n",
    "axes[0].scatter(centroids[:, 0], centroids[:, 1], c='red', marker='X', s=200,\n",
    "                edgecolors='black', linewidths=1.5)\n",
    "axes[0].set_title('K-Means Result (K=3)')\n",
    "axes[0].set_xlabel('Feature 1')\n",
    "axes[0].set_ylabel('Feature 2')\n",
    "\n",
    "# Right: Ground truth\n",
    "axes[1].scatter(X[:, 0], X[:, 1], c=y_true, cmap='viridis', s=30, alpha=0.7)\n",
    "axes[1].set_title('Ground Truth Labels')\n",
    "axes[1].set_xlabel('Feature 1')\n",
    "axes[1].set_ylabel('Feature 2')\n",
    "\n",
    "plt.suptitle('K-Means recovered the true clusters!', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Note: The cluster NUMBERS may differ (K-Means might call cluster 0 what the truth calls cluster 2),\")\n",
    "print(\"but the GROUPINGS should match.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Happens with the Wrong K?\n",
    "\n",
    "K-Means requires you to choose K upfront. Let's see what happens when we pick the wrong number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different values of K\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 4))\n",
    "\n",
    "for i, k in enumerate([2, 3, 4, 5]):\n",
    "    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    pred = km.fit_predict(X)\n",
    "\n",
    "    axes[i].scatter(X[:, 0], X[:, 1], c=pred, cmap='viridis', s=30, alpha=0.7)\n",
    "    axes[i].scatter(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1],\n",
    "                    c='red', marker='X', s=200, edgecolors='black', linewidths=1.5)\n",
    "    axes[i].set_title(f'K = {k}')\n",
    "    axes[i].set_xlabel('Feature 1')\n",
    "    axes[i].set_ylabel('Feature 2')\n",
    "\n",
    "plt.suptitle('Effect of Choosing Different K Values', fontsize=13, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"K=2 merges two clusters. K=3 is just right. K=4 and K=5 split clusters unnecessarily.\")\n",
    "print(\"How do we pick the right K? That's next.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluating Clusters: Elbow Method & Silhouette Score <a id=\"7-evaluating-clusters\"></a>\n",
    "\n",
    "Since we usually don't have ground truth labels in unsupervised learning, we need other ways to assess how good our clusters are and how to choose K."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7a. The Elbow Method\n",
    "\n",
    "The **Elbow Method** plots the **inertia** (within-cluster sum of squares) for different values of K. As K increases, inertia always decreases — but at some point the improvement becomes marginal. The \"elbow\" in the curve suggests a good K.\n",
    "\n",
    "Think of it this way: going from 1 cluster to 2 dramatically reduces inertia. Going from 2 to 3 helps a lot. Going from 3 to 4? Not much improvement — you've already captured the natural structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute inertia for K = 1 through 10\n",
    "inertias = []\n",
    "K_range = range(1, 11)\n",
    "\n",
    "for k in K_range:\n",
    "    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    km.fit(X)\n",
    "    inertias.append(km.inertia_)\n",
    "\n",
    "# Plot the elbow curve\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(K_range, inertias, 'bo-', linewidth=2, markersize=8)\n",
    "plt.axvline(x=3, color='red', linestyle='--', alpha=0.7, label='Elbow at K=3')\n",
    "plt.xlabel('Number of Clusters (K)', fontsize=12)\n",
    "plt.ylabel('Inertia (Within-Cluster Sum of Squares)', fontsize=12)\n",
    "plt.title('Elbow Method', fontsize=13, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.xticks(K_range)\n",
    "plt.show()\n",
    "\n",
    "print(\"The 'elbow' appears at K=3, confirming our synthetic data has 3 natural clusters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7b. Silhouette Score\n",
    "\n",
    "The **Silhouette Score** measures how similar each point is to its own cluster compared to other clusters. It ranges from **-1 to +1**:\n",
    "\n",
    "- **+1**: Points are well-matched to their cluster and poorly matched to neighbors\n",
    "- **0**: Points are on the boundary between clusters\n",
    "- **-1**: Points are likely assigned to the wrong cluster\n",
    "\n",
    "A higher average silhouette score means better-defined clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Compute silhouette score for K = 2 through 8\n",
    "sil_scores = []\n",
    "K_sil_range = range(2, 9)  # Silhouette requires at least 2 clusters\n",
    "\n",
    "for k in K_sil_range:\n",
    "    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels_k = km.fit_predict(X)\n",
    "    sil_scores.append(silhouette_score(X, labels_k))\n",
    "\n",
    "# Plot silhouette scores\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(K_sil_range, sil_scores, 'go-', linewidth=2, markersize=8)\n",
    "plt.axvline(x=3, color='red', linestyle='--', alpha=0.7, label='Best at K=3')\n",
    "plt.xlabel('Number of Clusters (K)', fontsize=12)\n",
    "plt.ylabel('Silhouette Score', fontsize=12)\n",
    "plt.title('Silhouette Score by K', fontsize=13, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.xticks(K_sil_range)\n",
    "plt.show()\n",
    "\n",
    "print(\"Silhouette scores by K:\")\n",
    "for k, score in zip(K_sil_range, sil_scores):\n",
    "    marker = \" <-- best\" if score == max(sil_scores) else \"\"\n",
    "    print(f\"  K={k}: {score:.4f}{marker}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key takeaway:** Both the Elbow Method and Silhouette Score point to K=3 for our synthetic data — exactly right. In practice, these methods don't always agree perfectly, and domain knowledge plays an important role in the final decision. Use them as **guides**, not gospel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. From Toy Data to Real Data: Feature Scaling <a id=\"8-toy-to-real\"></a>\n",
    "\n",
    "Our toy example worked perfectly because both features were on similar scales. Real baseball data is different — features live on wildly different scales:\n",
    "\n",
    "| Feature | Typical Range | Unit |\n",
    "|---------|--------------|------|\n",
    "| Pitch velocity | 70–100 | mph |\n",
    "| Spin rate | 1,500–3,000 | rpm |\n",
    "| Horizontal break | -2.0 to +2.0 | feet |\n",
    "| Vertical break | -2.0 to +2.0 | feet |\n",
    "\n",
    "K-Means uses **Euclidean distance** to assign points to clusters. Without scaling, a 100 rpm difference in spin rate would dominate a 5 mph difference in velocity — simply because the numbers are bigger. The algorithm would effectively ignore velocity.\n",
    "\n",
    "**The fix: StandardScaler.** This transforms each feature to have mean = 0 and standard deviation = 1. After scaling, a 1-standard-deviation change in velocity is treated the same as a 1-standard-deviation change in spin rate.\n",
    "\n",
    "**Rule of thumb:** Always scale your features before running K-Means.\n",
    "\n",
    "### Other Considerations for Real Data\n",
    "\n",
    "- **Higher dimensions**: Our toy example had 2 features we could plot directly. Real examples will have 4–10+ features. We'll use PCA to project clusters down to 2D for visualization. (PCA is covered in depth in Part 2.)\n",
    "- **Missing data**: K-Means cannot handle NaN values. We'll need to drop or impute missing rows.\n",
    "- **No ground truth**: In most real-world clustering, there are no labels to check against. Example 1 (Pitch Classification) is a lucky exception — Statcast labels every pitch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Demo: StandardScaler in action\n",
    "demo_df = pd.DataFrame({\n",
    "    'velocity_mph': [95.2, 88.1, 92.4, 79.3, 96.7],\n",
    "    'spin_rate_rpm': [2400, 2650, 2200, 2800, 2350]\n",
    "})\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled = scaler.fit_transform(demo_df)\n",
    "scaled_df = pd.DataFrame(scaled, columns=demo_df.columns)\n",
    "\n",
    "print(\"BEFORE scaling:\")\n",
    "print(demo_df.to_string(index=False))\n",
    "print(f\"\\n  velocity range: {demo_df['velocity_mph'].min():.1f} to {demo_df['velocity_mph'].max():.1f}\")\n",
    "print(f\"  spin rate range: {demo_df['spin_rate_rpm'].min():.0f} to {demo_df['spin_rate_rpm'].max():.0f}\")\n",
    "\n",
    "print(\"\\nAFTER scaling (mean=0, std=1):\")\n",
    "print(scaled_df.round(2).to_string(index=False))\n",
    "print(f\"\\n  velocity range: {scaled_df['velocity_mph'].min():.2f} to {scaled_df['velocity_mph'].max():.2f}\")\n",
    "print(f\"  spin rate range: {scaled_df['spin_rate_rpm'].min():.2f} to {scaled_df['spin_rate_rpm'].max():.2f}\")\n",
    "print(\"\\nNow both features are on the same scale!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Baseball Example 1: Pitch Classification <a id=\"9-pitch-classification\"></a>\n",
    "\n",
    "Can K-Means rediscover pitch types from raw tracking data? Statcast labels every pitch with a `pitch_type` (fastball, slider, curveball, etc.), but what if we stripped those labels away and asked the algorithm to find groups based only on physical characteristics?\n",
    "\n",
    "We'll use **Tarik Skubal's** 2024 season — the AL Cy Young winner we featured in the data extraction notebook. He throws 4 distinct pitch types which should give us a clear result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pybaseball import statcast_pitcher, playerid_lookup\n",
    "\n",
    "# Look up Tarik Skubal's player ID\n",
    "skubal_info = playerid_lookup('skubal', 'tarik')\n",
    "skubal_id = skubal_info['key_mlbam'].values[0]\n",
    "print(f\"Skubal's MLBAM ID: {skubal_id}\")\n",
    "\n",
    "# Pull his full 2024 season of Statcast data\n",
    "skubal_data = statcast_pitcher(\n",
    "    start_dt='2024-03-28',\n",
    "    end_dt='2024-09-29',\n",
    "    player_id=skubal_id\n",
    ")\n",
    "\n",
    "print(f\"\\nShape: {skubal_data.shape[0]:,} pitches x {skubal_data.shape[1]} columns\")\n",
    "skubal_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select our clustering features + the pitch_type label for later comparison\n",
    "feature_cols = ['release_speed', 'release_spin_rate', 'pfx_x', 'pfx_z']\n",
    "\n",
    "pitch_df = skubal_data[feature_cols + ['pitch_type']].copy()\n",
    "\n",
    "# Check missing values\n",
    "print(\"Missing values:\")\n",
    "print(pitch_df.isnull().sum())\n",
    "\n",
    "print(f\"\\nPitch type breakdown:\")\n",
    "print(pitch_df['pitch_type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: drop missing values and scale features\n",
    "pitch_clean = pitch_df.dropna(subset=feature_cols).copy()\n",
    "print(f\"After dropping NaN: {len(pitch_clean):,} pitches ({len(pitch_df) - len(pitch_clean)} removed)\")\n",
    "\n",
    "# Filter out very rare pitch types (fewer than 10 occurrences)\n",
    "type_counts = pitch_clean['pitch_type'].value_counts()\n",
    "common_types = type_counts[type_counts >= 10].index\n",
    "pitch_clean = pitch_clean[pitch_clean['pitch_type'].isin(common_types)]\n",
    "print(f\"After filtering rare pitch types: {len(pitch_clean):,} pitches\")\n",
    "print(f\"Pitch types kept: {list(common_types)}\")\n",
    "\n",
    "# Scale the features\n",
    "scaler_pitch = StandardScaler()\n",
    "X_pitch = scaler_pitch.fit_transform(pitch_clean[feature_cols])\n",
    "print(f\"\\nScaled feature matrix shape: {X_pitch.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine K using Elbow Method and Silhouette Score\n",
    "inertias_pitch = []\n",
    "sil_scores_pitch = []\n",
    "K_range_pitch = range(2, 9)\n",
    "\n",
    "for k in K_range_pitch:\n",
    "    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    pred = km.fit_predict(X_pitch)\n",
    "    inertias_pitch.append(km.inertia_)\n",
    "    sil_scores_pitch.append(silhouette_score(X_pitch, pred))\n",
    "\n",
    "# Plot both\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(K_range_pitch, inertias_pitch, 'bo-', linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel('Number of Clusters (K)')\n",
    "axes[0].set_ylabel('Inertia')\n",
    "axes[0].set_title('Elbow Method — Skubal Pitches')\n",
    "axes[0].set_xticks(list(K_range_pitch))\n",
    "\n",
    "axes[1].plot(K_range_pitch, sil_scores_pitch, 'go-', linewidth=2, markersize=8)\n",
    "axes[1].set_xlabel('Number of Clusters (K)')\n",
    "axes[1].set_ylabel('Silhouette Score')\n",
    "axes[1].set_title('Silhouette Score — Skubal Pitches')\n",
    "axes[1].set_xticks(list(K_range_pitch))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Silhouette scores:\")\n",
    "for k, score in zip(K_range_pitch, sil_scores_pitch):\n",
    "    print(f\"  K={k}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit K-Means with the number of pitch types we found\n",
    "n_pitch_types = len(common_types)\n",
    "print(f\"Using K={n_pitch_types} (matching the number of actual pitch types)\")\n",
    "\n",
    "kmeans_pitch = KMeans(n_clusters=n_pitch_types, random_state=42, n_init=10)\n",
    "pitch_clean['cluster'] = kmeans_pitch.fit_predict(X_pitch)\n",
    "\n",
    "print(f\"\\nPoints per cluster:\")\n",
    "print(pitch_clean['cluster'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 1: Velocity vs Spin Rate colored by cluster\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# K-Means clusters\n",
    "sns.scatterplot(data=pitch_clean, x='release_speed', y='release_spin_rate',\n",
    "                hue='cluster', palette='viridis', alpha=0.6, s=20, ax=axes[0], legend='full')\n",
    "axes[0].set_title('K-Means Clusters')\n",
    "axes[0].set_xlabel('Velocity (mph)')\n",
    "axes[0].set_ylabel('Spin Rate (rpm)')\n",
    "\n",
    "# Actual pitch types\n",
    "sns.scatterplot(data=pitch_clean, x='release_speed', y='release_spin_rate',\n",
    "                hue='pitch_type', palette='Set2', alpha=0.6, s=20, ax=axes[1], legend='full')\n",
    "axes[1].set_title('Actual Pitch Types')\n",
    "axes[1].set_xlabel('Velocity (mph)')\n",
    "axes[1].set_ylabel('Spin Rate (rpm)')\n",
    "\n",
    "plt.suptitle('Velocity vs. Spin Rate — Clusters vs. Reality', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 2: Pitch Movement Plot (pfx_x vs pfx_z) — the baseball classic\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# K-Means clusters\n",
    "sns.scatterplot(data=pitch_clean, x='pfx_x', y='pfx_z',\n",
    "                hue='cluster', palette='viridis', alpha=0.6, s=20, ax=axes[0], legend='full')\n",
    "axes[0].set_title('K-Means Clusters')\n",
    "axes[0].set_xlabel('Horizontal Break (ft)')\n",
    "axes[0].set_ylabel('Vertical Break (ft)')\n",
    "axes[0].axhline(y=0, color='gray', linestyle='--', alpha=0.3)\n",
    "axes[0].axvline(x=0, color='gray', linestyle='--', alpha=0.3)\n",
    "\n",
    "# Actual pitch types\n",
    "sns.scatterplot(data=pitch_clean, x='pfx_x', y='pfx_z',\n",
    "                hue='pitch_type', palette='Set2', alpha=0.6, s=20, ax=axes[1], legend='full')\n",
    "axes[1].set_title('Actual Pitch Types')\n",
    "axes[1].set_xlabel('Horizontal Break (ft)')\n",
    "axes[1].set_ylabel('Vertical Break (ft)')\n",
    "axes[1].axhline(y=0, color='gray', linestyle='--', alpha=0.3)\n",
    "axes[1].axvline(x=0, color='gray', linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.suptitle('Pitch Movement — Clusters vs. Reality', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"This is the classic 'pitch movement' plot. Each clump represents a different pitch type.\")\n",
    "print(\"Compare left vs. right — K-Means found the same groups!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive plotly scatter — hover over any pitch to see details\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter(\n",
    "    pitch_clean,\n",
    "    x='pfx_x', y='pfx_z',\n",
    "    color='cluster',\n",
    "    hover_data=['pitch_type', 'release_speed', 'release_spin_rate'],\n",
    "    title='Interactive: Pitch Movement Colored by K-Means Cluster',\n",
    "    labels={\n",
    "        'pfx_x': 'Horizontal Break (ft)',\n",
    "        'pfx_z': 'Vertical Break (ft)',\n",
    "        'release_speed': 'Velocity (mph)',\n",
    "        'release_spin_rate': 'Spin Rate (rpm)',\n",
    "        'cluster': 'Cluster'\n",
    "    },\n",
    "    opacity=0.6,\n",
    "    color_continuous_scale='Viridis'\n",
    ")\n",
    "fig.update_layout(width=700, height=550)\n",
    "fig.show()\n",
    "\n",
    "print(\"Hover over any point to see its actual pitch type, velocity, and spin rate!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How well did K-Means match reality? Cross-tabulation\n",
    "ct = pd.crosstab(pitch_clean['pitch_type'], pitch_clean['cluster'], margins=True)\n",
    "print(\"Cross-tabulation: Actual Pitch Type vs. K-Means Cluster\\n\")\n",
    "print(ct)\n",
    "\n",
    "# Compute \"accuracy\" by mapping each cluster to its majority pitch type\n",
    "print(\"\\n--- Cluster-to-Pitch-Type Mapping ---\")\n",
    "correct = 0\n",
    "for cluster_id in range(n_pitch_types):\n",
    "    cluster_mask = pitch_clean['cluster'] == cluster_id\n",
    "    majority_type = pitch_clean.loc[cluster_mask, 'pitch_type'].mode()[0]\n",
    "    count_correct = (pitch_clean.loc[cluster_mask, 'pitch_type'] == majority_type).sum()\n",
    "    count_total = cluster_mask.sum()\n",
    "    correct += count_correct\n",
    "    print(f\"  Cluster {cluster_id} -> {majority_type} ({count_correct}/{count_total} = {count_correct/count_total:.1%})\")\n",
    "\n",
    "overall_acc = correct / len(pitch_clean)\n",
    "print(f\"\\nOverall accuracy: {correct}/{len(pitch_clean)} = {overall_acc:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key takeaway:** K-Means was able to rediscover Skubal's pitch types purely from physical measurements — velocity, spin rate, and movement — with no labels provided. This is the power of clustering: finding natural structure in data. Where the algorithm struggles (e.g., separating fastballs from sinkers) reflects genuine physical similarity between those pitch types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Baseball Example 2: Pitcher Archetypes <a id=\"10-pitcher-archetypes\"></a>\n",
    "\n",
    "Instead of clustering individual pitches, let's zoom out and cluster **pitchers**. Every pitcher has a unique profile: some rely on a dominant fastball with elite velocity, others use a diverse arsenal of breaking balls. Can K-Means discover natural pitcher archetypes from season-level stats?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pybaseball import pitching_stats\n",
    "\n",
    "# Get 2024 pitching stats from FanGraphs\n",
    "# qual=50 gives us pitchers with at least 50 IP for a broader sample\n",
    "pitching_2024 = pitching_stats(2024, qual=50)\n",
    "\n",
    "print(f\"Shape: {pitching_2024.shape[0]} pitchers x {pitching_2024.shape[1]} columns\")\n",
    "pitching_2024.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore available columns related to our features of interest\n",
    "# We want: velocity, strikeout/walk rates, batted ball profile, pitch mix\n",
    "all_cols = pitching_2024.columns.tolist()\n",
    "\n",
    "# Print columns that might contain our desired features\n",
    "print(\"Velocity-related columns:\")\n",
    "print([c for c in all_cols if 'vFA' in c or 'FBv' in c or 'Velocity' in c or c.startswith('v')][:15])\n",
    "\n",
    "print(\"\\nRate columns (K%, BB%, etc.):\")\n",
    "print([c for c in all_cols if '%' in c][:20])\n",
    "\n",
    "print(\"\\nPitch mix / usage columns:\")\n",
    "print([c for c in all_cols if c.endswith('%') and len(c) <= 5][:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for clustering\n",
    "# We'll pick features that capture velocity, command, contact quality, and pitch mix\n",
    "# NOTE: Column names may vary slightly across pybaseball versions.\n",
    "# If a column is missing, check the printed columns above and adjust.\n",
    "\n",
    "pitcher_feature_cols = [\n",
    "    'K%',       # Strikeout rate -- swing-and-miss ability\n",
    "    'BB%',      # Walk rate -- command/control\n",
    "    'GB%',      # Ground ball rate -- contact management\n",
    "    'HR/FB',    # Home runs per fly ball -- fly ball damage\n",
    "    'SwStr%',   # Swinging strike rate -- stuff quality\n",
    "]\n",
    "\n",
    "# Check which columns actually exist and add velocity if available\n",
    "for vel_col in ['vFA (pi)', 'FBv', 'vFA']:\n",
    "    if vel_col in pitching_2024.columns:\n",
    "        pitcher_feature_cols.insert(0, vel_col)\n",
    "        print(f\"Using '{vel_col}' for fastball velocity\")\n",
    "        break\n",
    "\n",
    "# Verify all selected columns exist\n",
    "missing = [c for c in pitcher_feature_cols if c not in pitching_2024.columns]\n",
    "if missing:\n",
    "    print(f\"WARNING: Missing columns: {missing}\")\n",
    "    pitcher_feature_cols = [c for c in pitcher_feature_cols if c in pitching_2024.columns]\n",
    "\n",
    "print(f\"\\nFinal feature set ({len(pitcher_feature_cols)} features): {pitcher_feature_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to starting pitchers for a more homogeneous comparison\n",
    "starters = pitching_2024[pitching_2024['GS'] >= 10].copy()\n",
    "print(f\"Starting pitchers (GS >= 10): {len(starters)}\")\n",
    "\n",
    "# Preprocessing: drop missing values and scale\n",
    "pitcher_clean = starters[['Name', 'Team', 'WAR', 'ERA', 'IP'] + pitcher_feature_cols].dropna(subset=pitcher_feature_cols).copy()\n",
    "print(f\"After dropping NaN: {len(pitcher_clean)} pitchers\")\n",
    "\n",
    "scaler_pitcher = StandardScaler()\n",
    "X_pitcher = scaler_pitcher.fit_transform(pitcher_clean[pitcher_feature_cols])\n",
    "print(f\"Scaled feature matrix: {X_pitcher.shape}\")\n",
    "\n",
    "# Quick look at the data\n",
    "pitcher_clean[['Name', 'Team', 'WAR'] + pitcher_feature_cols].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine K with Elbow and Silhouette\n",
    "inertias_p = []\n",
    "sil_scores_p = []\n",
    "K_range_p = range(2, 9)\n",
    "\n",
    "for k in K_range_p:\n",
    "    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    pred = km.fit_predict(X_pitcher)\n",
    "    inertias_p.append(km.inertia_)\n",
    "    sil_scores_p.append(silhouette_score(X_pitcher, pred))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(K_range_p, inertias_p, 'bo-', linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel('Number of Clusters (K)')\n",
    "axes[0].set_ylabel('Inertia')\n",
    "axes[0].set_title('Elbow Method — Pitcher Archetypes')\n",
    "axes[0].set_xticks(list(K_range_p))\n",
    "\n",
    "axes[1].plot(K_range_p, sil_scores_p, 'go-', linewidth=2, markersize=8)\n",
    "axes[1].set_xlabel('Number of Clusters (K)')\n",
    "axes[1].set_ylabel('Silhouette Score')\n",
    "axes[1].set_title('Silhouette Score — Pitcher Archetypes')\n",
    "axes[1].set_xticks(list(K_range_p))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Silhouette scores:\")\n",
    "for k, score in zip(K_range_p, sil_scores_p):\n",
    "    print(f\"  K={k}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit K-Means — choose K based on the elbow/silhouette plots above\n",
    "# We'll use K=4 as a reasonable default; adjust if your plots suggest otherwise\n",
    "K_pitcher = 4\n",
    "kmeans_pitcher = KMeans(n_clusters=K_pitcher, random_state=42, n_init=10)\n",
    "pitcher_clean['archetype'] = kmeans_pitcher.fit_predict(X_pitcher)\n",
    "\n",
    "print(f\"Using K={K_pitcher}\")\n",
    "print(f\"\\nPitchers per archetype:\")\n",
    "print(pitcher_clean['archetype'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 1: Cluster profile heatmap\n",
    "# Shows the mean (scaled) feature value for each archetype\n",
    "cluster_means = pitcher_clean.groupby('archetype')[pitcher_feature_cols].mean()\n",
    "\n",
    "# Scale the means for better heatmap visualization\n",
    "cluster_means_scaled = pd.DataFrame(\n",
    "    StandardScaler().fit_transform(cluster_means),\n",
    "    index=cluster_means.index,\n",
    "    columns=cluster_means.columns\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.heatmap(cluster_means_scaled, annot=cluster_means.round(1).values, fmt='',\n",
    "            cmap='RdYlBu_r', center=0, linewidths=0.5,\n",
    "            yticklabels=[f'Archetype {i}' for i in range(K_pitcher)])\n",
    "plt.title('Pitcher Archetype Profiles (values = raw means, colors = relative scale)', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Each row is an archetype. Warm colors = above average, cool colors = below average.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 2: PCA projection to 2D for an interactive scatter\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Project the pitcher features down to 2 dimensions\n",
    "# (We'll cover PCA in detail in Part 2 -- for now, think of it as\n",
    "#  compressing our features into 2 axes that capture the most variation)\n",
    "pca_pitcher = PCA(n_components=2)\n",
    "X_pitcher_2d = pca_pitcher.fit_transform(X_pitcher)\n",
    "\n",
    "pitcher_clean['PC1'] = X_pitcher_2d[:, 0]\n",
    "pitcher_clean['PC2'] = X_pitcher_2d[:, 1]\n",
    "\n",
    "fig = px.scatter(\n",
    "    pitcher_clean,\n",
    "    x='PC1', y='PC2',\n",
    "    color='archetype',\n",
    "    hover_data=['Name', 'Team', 'WAR', 'ERA'],\n",
    "    title='Pitcher Archetypes — PCA Projection (hover for names)',\n",
    "    labels={'PC1': f'PC1 ({pca_pitcher.explained_variance_ratio_[0]:.1%} variance)',\n",
    "            'PC2': f'PC2 ({pca_pitcher.explained_variance_ratio_[1]:.1%} variance)'},\n",
    "    opacity=0.7,\n",
    ")\n",
    "fig.update_layout(width=750, height=550)\n",
    "fig.show()\n",
    "\n",
    "print(f\"PCA explains {pca_pitcher.explained_variance_ratio_.sum():.1%} of total variance in 2 components.\")\n",
    "print(\"Hover over any point to see the pitcher's name, team, WAR, and ERA!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpret archetypes: top pitchers in each cluster\n",
    "print(\"=\" * 70)\n",
    "print(\"PITCHER ARCHETYPES — Top 5 by WAR in each cluster\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for archetype_id in range(K_pitcher):\n",
    "    members = pitcher_clean[pitcher_clean['archetype'] == archetype_id]\n",
    "\n",
    "    # Summarize the archetype's profile\n",
    "    profile = members[pitcher_feature_cols].mean()\n",
    "\n",
    "    print(f\"\\n--- Archetype {archetype_id} ({len(members)} pitchers) ---\")\n",
    "    print(f\"Profile: {', '.join(f'{col}={val:.1f}' for col, val in profile.items())}\")\n",
    "    print()\n",
    "    print(members.nlargest(5, 'WAR')[['Name', 'Team', 'WAR', 'ERA'] + pitcher_feature_cols].to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Look at each archetype's profile and notable pitchers.\")\n",
    "print(\"Can you give each archetype a descriptive name? (e.g., 'Power Arms', 'Craftsmen', 'Groundballers')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key takeaway:** K-Means can discover natural pitcher archetypes from season-level statistics. Unlike the pitch classification example, there is no \"ground truth\" here — the value comes from the archetypes themselves and the insights they provide about different pitching profiles. The heatmap and archetype members help us interpret and name the clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Baseball Example 3: Team Playing Styles <a id=\"11-team-playing-styles\"></a>\n",
    "\n",
    "Finally, let's zoom all the way out and cluster **entire teams**. Do teams have distinct strategic identities? With only 30 MLB teams, every cluster will contain recognizable names — making the results easy to discuss and debate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pybaseball import team_batting, team_pitching\n",
    "\n",
    "# Pull 2024 team-level stats\n",
    "team_bat = team_batting(2024)\n",
    "team_pitch = team_pitching(2024)\n",
    "\n",
    "print(f\"Team batting: {team_bat.shape}\")\n",
    "print(f\"Team pitching: {team_pitch.shape}\")\n",
    "\n",
    "team_bat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge batting and pitching on team\n",
    "# Rename colliding columns to avoid confusion\n",
    "team_bat_renamed = team_bat.rename(columns={'K%': 'K%_bat', 'BB%': 'BB%_bat'})\n",
    "team_pitch_renamed = team_pitch.rename(columns={'K%': 'K%_pitch', 'BB%': 'BB%_pitch'})\n",
    "\n",
    "# Find the common team identifier column\n",
    "# (pybaseball may use 'Team' or 'teamID' -- let's check)\n",
    "print(\"Batting columns:\", team_bat_renamed.columns[:10].tolist())\n",
    "print(\"Pitching columns:\", team_pitch_renamed.columns[:10].tolist())\n",
    "\n",
    "# Merge on the team name column\n",
    "team_col = 'Team' if 'Team' in team_bat_renamed.columns else team_bat_renamed.columns[0]\n",
    "teams = team_bat_renamed.merge(team_pitch_renamed, on=team_col, suffixes=('_bat', '_pitch'))\n",
    "print(f\"\\nMerged teams: {len(teams)} rows\")\n",
    "teams.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for team clustering\n",
    "# We want a mix of offensive and pitching characteristics\n",
    "team_feature_candidates = {\n",
    "    # Offense\n",
    "    'HR': 'HR',\n",
    "    'SB': 'SB',\n",
    "    'K%_bat': 'K%_bat',\n",
    "    'BB%_bat': 'BB%_bat',\n",
    "    'OPS': 'OPS',\n",
    "    # Pitching\n",
    "    'ERA': 'ERA',\n",
    "    'K%_pitch': 'K%_pitch',\n",
    "    'BB%_pitch': 'BB%_pitch',\n",
    "}\n",
    "\n",
    "# Use whichever columns exist in the merged dataframe\n",
    "team_feature_cols = [c for c in team_feature_candidates.values() if c in teams.columns]\n",
    "print(f\"Team features ({len(team_feature_cols)}): {team_feature_cols}\")\n",
    "\n",
    "# Check for missing values\n",
    "team_clean = teams[[team_col] + team_feature_cols].dropna(subset=team_feature_cols).copy()\n",
    "print(f\"Teams with complete data: {len(team_clean)}\")\n",
    "\n",
    "# Scale\n",
    "scaler_team = StandardScaler()\n",
    "X_team = scaler_team.fit_transform(team_clean[team_feature_cols])\n",
    "print(f\"Scaled feature matrix: {X_team.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With only 30 teams, we'll skip the elbow/silhouette (already demonstrated)\n",
    "# and jump straight to K=4 for interpretability\n",
    "K_team = 4\n",
    "kmeans_team = KMeans(n_clusters=K_team, random_state=42, n_init=10)\n",
    "team_clean['cluster'] = kmeans_team.fit_predict(X_team)\n",
    "\n",
    "# PCA for 2D visualization\n",
    "pca_team = PCA(n_components=2)\n",
    "X_team_2d = pca_team.fit_transform(X_team)\n",
    "team_clean['PC1'] = X_team_2d[:, 0]\n",
    "team_clean['PC2'] = X_team_2d[:, 1]\n",
    "\n",
    "# Interactive scatter — find your favorite team!\n",
    "fig = px.scatter(\n",
    "    team_clean,\n",
    "    x='PC1', y='PC2',\n",
    "    color='cluster',\n",
    "    text=team_col,\n",
    "    hover_data=team_feature_cols,\n",
    "    title='MLB Team Playing Styles (2024) — Find Your Team!',\n",
    "    labels={'PC1': f'PC1 ({pca_team.explained_variance_ratio_[0]:.1%} variance)',\n",
    "            'PC2': f'PC2 ({pca_team.explained_variance_ratio_[1]:.1%} variance)'},\n",
    "    opacity=0.8,\n",
    ")\n",
    "fig.update_traces(textposition='top center', textfont_size=9)\n",
    "fig.update_layout(width=800, height=600)\n",
    "fig.show()\n",
    "\n",
    "print(\"Hover over any team to see their stats. Teams in the same cluster have similar profiles.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster profiles and team membership\n",
    "print(\"=\" * 60)\n",
    "print(\"TEAM PLAYING STYLE CLUSTERS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for cluster_id in range(K_team):\n",
    "    members = team_clean[team_clean['cluster'] == cluster_id]\n",
    "    profile = members[team_feature_cols].mean()\n",
    "\n",
    "    print(f\"\\n--- Cluster {cluster_id} ({len(members)} teams) ---\")\n",
    "    print(f\"Teams: {', '.join(members[team_col].values)}\")\n",
    "    print(f\"Avg profile:\")\n",
    "    for col in team_feature_cols:\n",
    "        val = profile[col]\n",
    "        print(f\"  {col:15s} {val:.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Can you name each cluster? (e.g., 'Power Offenses', 'Pitching-First', 'Rebuilding')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key takeaway:** Even with just 30 data points, K-Means can identify meaningful team playing styles. The same clustering workflow — select features, scale, fit, visualize, interpret — works at every level of granularity: individual pitches, player careers, and team identities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary & Next Steps <a id=\"12-summary\"></a>\n",
    "\n",
    "### What We Covered\n",
    "\n",
    "| Example | Data Source | Features | K | Key Finding |\n",
    "|---------|------------|----------|---|-------------|\n",
    "| Pitch Classification | `statcast_pitcher()` | velocity, spin, movement | ~4 | K-Means recovered actual pitch types from raw data |\n",
    "| Pitcher Archetypes | `pitching_stats()` | velocity, rates, pitch mix | ~4 | Natural groupings like Power Arms and Craftsmen |\n",
    "| Team Playing Styles | `team_batting()` + `team_pitching()` | offense + pitching stats | 4 | Teams cluster by strategic identity |\n",
    "\n",
    "### Key Lessons\n",
    "\n",
    "1. **K-Means is a powerful first-pass tool** for discovering structure in data\n",
    "2. **Feature scaling is critical** — always use `StandardScaler` before K-Means\n",
    "3. **The Elbow Method and Silhouette Score** help choose K, but domain knowledge is equally important\n",
    "4. **Clustering is unsupervised** — there's rarely a \"right answer,\" and interpretation requires context\n",
    "5. **The same workflow applies at every level** — from individual pitches to entire teams\n",
    "\n",
    "### What's Next\n",
    "\n",
    "**Advanced Clustering** (`advanced_clustering.ipynb`):\n",
    "- **DBSCAN**: A clustering algorithm that doesn't require specifying K and can find oddly shaped clusters and outliers\n",
    "- **Hierarchical Clustering**: Builds a tree (dendrogram) of nested clusters, giving you flexibility in choosing the number of groups\n",
    "- **Gaussian Mixture Models**: A probabilistic approach that assigns soft/fuzzy cluster memberships instead of hard assignments\n",
    "- **Advanced evaluation**: Davies-Bouldin index, Calinski-Harabasz score, Gap Statistic\n",
    "\n",
    "**Unsupervised Learning Part 2 — Dimensionality Reduction**:\n",
    "- **PCA**: Compress many features into fewer components (we used it briefly for visualization here)\n",
    "- **t-SNE and UMAP**: Non-linear reduction techniques for beautiful 2D visualizations\n",
    "- **Combining reduction + clustering**: Reduce first, then cluster for better results\n",
    "\n",
    "Happy clustering!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Manipulation with Pandas & NumPy\n",
    "\n",
    "**Baseball Data Science Series — Notebook 02**"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Overview\n\n**Pandas** is the most important Python library for data analysis. If you want to work with baseball statistics — or any tabular data — pandas is your go-to tool. It lets you load, clean, filter, transform, and summarize data in just a few lines of code.\n\nIn this notebook, you will learn how to:\n\n- Use **NumPy** for fast numerical operations on arrays\n- Create and manipulate **pandas Series** (1D) and **DataFrames** (2D)\n- Select, filter, and modify data\n- Sort, group, and aggregate statistics\n- Combine multiple datasets together\n\nWe will use **real-style baseball batting statistics** throughout every example — no generic \"Hello World\" data here.\n\n### Prerequisites\n\nYou should have completed **`01_intro_to_python.ipynb`** before starting this notebook. You should be comfortable with:\n- Variables, data types, and basic operators\n- Lists and dictionaries\n- Loops and conditional statements\n- Functions",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Table of Contents\n\n1. [NumPy Basics](#1.-NumPy-Basics)\n2. [Pandas Series](#2.-Pandas-Series)\n3. [Pandas DataFrames — Creating & Loading](#3.-Pandas-DataFrames-—-Creating-&-Loading)\n4. [Selecting & Filtering Data](#4.-Selecting-&-Filtering-Data)\n5. [Modifying Data](#5.-Modifying-Data)\n6. [Sorting & Grouping](#6.-Sorting-&-Grouping)\n7. [Combining DataFrames](#7.-Combining-DataFrames)\n8. [Wrap-Up](#8.-Wrap-Up)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## 1. NumPy Basics\n\n**NumPy** (Numerical Python) is the foundation for numerical computing in Python. It provides:\n\n- **Fast array operations** — much faster than regular Python lists\n- **Mathematical functions** — mean, standard deviation, etc.\n- **The backbone of pandas** — every pandas DataFrame is built on NumPy arrays under the hood\n\nWe will keep this section brief since pandas is our main focus. But understanding NumPy basics will help you understand how pandas works and give you tools for quick numerical calculations.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# First, let's import our libraries\n# By convention, numpy is imported as 'np' and pandas as 'pd'\nimport numpy as np\nimport pandas as pd\n\nprint(f\"NumPy version: {np.__version__}\")\nprint(f\"Pandas version: {pd.__version__}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Creating NumPy Arrays\n\nA NumPy **array** is like a Python list, but optimized for numerical operations. There are several ways to create one:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Creating arrays from Python lists\n# Home run totals for 5 players\nhome_runs = np.array([58, 54, 39, 41, 22])\nprint(\"Home runs array:\", home_runs)\nprint(\"Type:\", type(home_runs))\nprint()\n\n# np.zeros() — create an array of all zeros (e.g., a fresh scoreboard)\nscoreboard = np.zeros(9)  # 9 innings, all zeros\nprint(\"Empty scoreboard (9 innings):\", scoreboard)\nprint()\n\n# np.ones() — create an array of all ones\nones = np.ones(5)\nprint(\"Array of ones:\", ones)\nprint()\n\n# np.arange() — create a range of numbers (like Python's range(), but returns an array)\ninnings = np.arange(1, 10)  # Innings 1 through 9\nprint(\"Innings:\", innings)\nprint()\n\n# np.linspace() — create evenly spaced numbers between two values\n# Useful for creating pitch speed ranges (e.g., for plotting later)\nspeed_range = np.linspace(70, 100, 7)  # 7 evenly spaced values from 70 to 100 mph\nprint(\"Pitch speed range (mph):\", speed_range)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Array Operations & Broadcasting\n\nOne of NumPy's superpowers is **element-wise operations** — you can do math on entire arrays at once, without writing loops. NumPy also supports **broadcasting**, which lets you combine arrays of different sizes in intuitive ways.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Element-wise operations on arrays\n# Pitch velocities in mph for a pitcher's last 6 fastballs\npitch_speeds_mph = np.array([95.2, 96.8, 94.1, 97.3, 93.5, 96.0])\n\n# Broadcasting: convert ALL speeds from mph to km/h by multiplying by a single number\n# NumPy applies the operation to every element automatically\npitch_speeds_kmh = pitch_speeds_mph * 1.60934\nprint(\"Pitch speeds (mph):\", pitch_speeds_mph)\nprint(\"Pitch speeds (km/h):\", np.round(pitch_speeds_kmh, 1))\nprint()\n\n# Element-wise math between two arrays\n# Hits and At Bats for 5 players\nhits = np.array([183, 197, 167, 166, 163])\nat_bats = np.array([570, 636, 545, 568, 580])\n\n# Calculate batting average for all 5 players at once\nbatting_avg = hits / at_bats\nprint(\"Hits:        \", hits)\nprint(\"At Bats:     \", at_bats)\nprint(\"Batting Avg: \", np.round(batting_avg, 3))\nprint()\n\n# More element-wise operations\nprint(\"Home runs doubled:\", home_runs * 2)\nprint(\"Home runs + 10:   \", home_runs + 10)\nprint(\"HR above 40?:     \", home_runs > 40)  # Returns a boolean array!",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Useful NumPy Functions\n\nNumPy has built-in functions for common statistical calculations. These are fast and convenient for quick analysis.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Sample data: RBI totals for 10 players\nrbi_totals = np.array([144, 130, 98, 109, 89, 100, 100, 91, 97, 75])\nplayer_names = ['Judge', 'Ohtani', 'Betts', 'Soto', 'Freeman',\n                'Seager', 'Semien', 'Devers', 'Guerrero Jr.', 'Turner']\n\nprint(\"RBI Totals:\", rbi_totals)\nprint()\n\n# Common statistical functions\nprint(f\"Total RBIs (sum):     {np.sum(rbi_totals)}\")\nprint(f\"Average RBIs (mean):  {np.mean(rbi_totals):.1f}\")\nprint(f\"Std Deviation:        {np.std(rbi_totals):.1f}\")\nprint(f\"Max RBIs:             {np.max(rbi_totals)}  ({player_names[np.argmax(rbi_totals)]})\")\nprint(f\"Min RBIs:             {np.min(rbi_totals)}  ({player_names[np.argmin(rbi_totals)]})\")\nprint(f\"Median RBIs:          {np.median(rbi_totals):.1f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "> **Key takeaway:** NumPy is powerful for raw numerical arrays, but it lacks labels and mixed data types. That is where **pandas** comes in. Pandas is built *on top of* NumPy, combining its speed with labeled rows/columns and support for mixed data types (numbers, strings, dates, etc.). For the rest of this notebook, we will focus on pandas.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## 2. Pandas Series\n\nA **pandas Series** is a one-dimensional labeled array. Think of it as a single column from a spreadsheet — it has values *and* an index (labels for each row).\n\nA Series is the building block of a DataFrame (which we will cover next). Each column in a DataFrame is actually a Series.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Creating a Series from a list\n# Batting averages for 5 players (default integer index: 0, 1, 2, ...)\navg_list = pd.Series([.321, .310, .307, .292, .281])\nprint(\"Series from a list:\")\nprint(avg_list)\nprint()\n\n# Creating a Series from a dictionary — the keys become the index labels\n# This is more useful because each value has a meaningful label\navg_dict = pd.Series({\n    'Aaron Judge': .321,\n    'Shohei Ohtani': .310,\n    'Mookie Betts': .307,\n    'Juan Soto': .292,\n    'Freddie Freeman': .281\n})\nprint(\"Series from a dictionary:\")\nprint(avg_dict)\nprint()\n\n# You can also name the Series\navg_dict.name = 'Batting Average'\nprint(f\"Series name: {avg_dict.name}\")\nprint(f\"Data type:   {avg_dict.dtype}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Indexing, Slicing & Operations on a Series\n\nYou can access values in a Series by position (like a list) or by label (like a dictionary). You can also perform math and comparisons on the entire Series at once.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Access by label (like a dictionary)\nprint(\"Judge's batting average:\", avg_dict['Aaron Judge'])\nprint()\n\n# Access by position (like a list) using .iloc[]\nprint(\"First player's average:\", avg_dict.iloc[0])\nprint(\"Last player's average: \", avg_dict.iloc[-1])\nprint()\n\n# Slicing — get multiple values\nprint(\"Top 3 batting averages:\")\nprint(avg_dict.iloc[:3])\nprint()\n\n# Boolean filtering — who is batting over .300?\nprint(\"Players batting over .300:\")\nprint(avg_dict[avg_dict > .300])\nprint()\n\n# Series math — quick calculations\nprint(f\"Mean batting average:    {avg_dict.mean():.3f}\")\nprint(f\"Highest batting average: {avg_dict.max():.3f} ({avg_dict.idxmax()})\")\nprint(f\"Sum of all averages:     {avg_dict.sum():.3f}\")\n\n# Sorting\nprint(\"\\nSorted (highest to lowest):\")\nprint(avg_dict.sort_values(ascending=False))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## 3. Pandas DataFrames — Creating & Loading\n\nA **DataFrame** is the core data structure in pandas. Think of it as a spreadsheet or database table:\n\n- It has **rows** (one per observation — e.g., one per player)\n- It has **columns** (one per variable — e.g., Player, Team, HR, AVG)\n- Each column is a **Series**\n\nThis is where pandas really shines. Let's build a DataFrame of batting statistics and learn how to explore it.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Creating a DataFrame from a dictionary\n# Each key becomes a column name, each value is a list of data for that column\n# This dataset represents batting stats for 10 top MLB hitters\n\ndata = {\n    'Player': ['Aaron Judge', 'Shohei Ohtani', 'Mookie Betts', 'Juan Soto', 'Freddie Freeman',\n               'Corey Seager', 'Marcus Semien', 'Rafael Devers', 'Vladimir Guerrero Jr.', 'Trea Turner'],\n    'Team': ['NYY', 'LAD', 'LAD', 'NYY', 'LAD', 'TEX', 'TEX', 'BOS', 'TOR', 'PHI'],\n    'Position': ['RF', 'DH', 'SS', 'LF', '1B', 'SS', '2B', '3B', '1B', 'SS'],\n    'G': [158, 159, 147, 157, 152, 153, 162, 154, 159, 155],\n    'AB': [570, 636, 545, 568, 580, 578, 620, 590, 604, 586],\n    'H': [183, 197, 167, 166, 163, 170, 174, 175, 182, 159],\n    'HR': [58, 54, 39, 41, 22, 33, 29, 28, 30, 21],\n    'RBI': [144, 130, 98, 109, 89, 100, 100, 91, 97, 75],\n    'BB': [78, 81, 65, 129, 62, 55, 50, 61, 57, 41],\n    'SO': [175, 162, 105, 141, 113, 136, 133, 127, 117, 134],\n    'AVG': [.321, .310, .307, .292, .281, .294, .281, .297, .301, .271]\n}\n\ndf = pd.DataFrame(data)\n\n# Display the full DataFrame\ndf",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Exploring a DataFrame\n\nWhen you first load or create a DataFrame, you should always explore it to understand its structure, size, and content. Here are the essential methods:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# .head() — view the first N rows (default is 5)\nprint(\"=== First 5 rows (.head()) ===\")\ndf.head()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# .tail() — view the last N rows (default is 5)\nprint(\"=== Last 3 rows (.tail(3)) ===\")\ndf.tail(3)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# .shape — how many rows and columns? (returns a tuple)\nprint(f\"Shape: {df.shape}\")\nprint(f\"  -> {df.shape[0]} rows (players)\")\nprint(f\"  -> {df.shape[1]} columns (stats)\")\nprint()\n\n# .columns — list all column names\nprint(f\"Columns: {list(df.columns)}\")\nprint()\n\n# .dtypes — what data type is each column?\nprint(\"Data types:\")\nprint(df.dtypes)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# .info() — a comprehensive summary of the DataFrame\n# Shows column names, non-null counts, and data types\ndf.info()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# .describe() — statistical summary of all numeric columns\n# Shows count, mean, std, min, 25%, 50%, 75%, max\ndf.describe()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "> **Loading data from CSV files:** In real projects, you will rarely type data by hand. Instead, you will load it from files using `pd.read_csv('filename.csv')`. This returns a DataFrame just like the one we built above. We will use `read_csv()` extensively in later notebooks in this series. For now, our inline data is perfect for learning the fundamentals.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## 4. Selecting & Filtering Data\n\nNow that we have a DataFrame, the most common thing you will do is **select specific columns** and **filter rows** based on conditions. This is like writing SQL WHERE clauses, but in Python.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Selecting a single column — returns a Series\nprint(\"=== Single column (Series) ===\")\nprint(df['Player'])\nprint()\nprint(f\"Type: {type(df['Player'])}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Selecting multiple columns — use a list of column names inside the brackets\n# This returns a new DataFrame (not a Series)\nprint(\"=== Multiple columns (DataFrame) ===\")\ndf[['Player', 'HR', 'AVG']]",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Row selection by POSITION using .iloc[] (integer location)\n# .iloc[] uses integer indices, like a Python list\n\nprint(\"=== Row selection with .iloc[] ===\")\nprint()\n\n# Get the first row (index 0)\nprint(\"First player (row 0):\")\nprint(df.iloc[0])\nprint()\n\n# Get rows 2 through 4 (indices 2, 3, 4)\nprint(\"Rows 2-4:\")\ndf.iloc[2:5]",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Row selection by LABEL using .loc[]\n# .loc[] uses the index labels (and also supports column names)\n\n# Select specific rows and columns by label\n# Here we use the default numeric index as our label\nprint(\"=== Row + Column selection with .loc[] ===\")\nprint()\n\n# Get row 0, but only the Player and HR columns\nprint(\"Row 0, specific columns:\")\nprint(df.loc[0, ['Player', 'HR', 'Team']])\nprint()\n\n# Get rows 0-2 with specific columns\nprint(\"Rows 0-2, selected columns:\")\ndf.loc[0:2, ['Player', 'Team', 'HR', 'RBI', 'AVG']]",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Boolean filtering — the most powerful way to filter data\n# \"Show me all players with more than 30 home runs\"\nprint(\"=== Players with more than 30 HR ===\")\ndf[df['HR'] > 30]",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Multiple conditions — use & (and), | (or)\n# IMPORTANT: each condition must be wrapped in parentheses!\n# \"Show me players with 30+ HR AND batting over .300\"\nprint(\"=== Players with 30+ HR AND AVG > .300 ===\")\ndf[(df['HR'] > 30) & (df['AVG'] > .300)]",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# .query() — a cleaner alternative for complex filters\n# Instead of df[(df['HR'] > 30) & (df['AVG'] > .300)], you can write:\nprint(\"=== Same filter using .query() ===\")\ndf.query('HR > 30 and AVG > .300')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## 5. Modifying Data\n\nNow let's learn how to **add new columns**, **rename columns**, **transform data**, and **handle missing values**. These are everyday tasks in data analysis.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Adding a new column — calculated from existing columns\n# OBP (On-Base Percentage) estimate: (H + BB) / (AB + BB)\n# Note: This is a simplified estimate — real OBP also includes HBP and SF\ndf['OBP_est'] = (df['H'] + df['BB']) / (df['AB'] + df['BB'])\ndf['OBP_est'] = df['OBP_est'].round(3)\n\n# AB per HR — how many at-bats between each home run?\ndf['AB_per_HR'] = (df['AB'] / df['HR']).round(1)\n\nprint(\"DataFrame with new columns:\")\ndf[['Player', 'AVG', 'OBP_est', 'HR', 'AB_per_HR']]",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Renaming columns with .rename()\n# Useful for making column names more descriptive\ndf_renamed = df.rename(columns={\n    'G': 'Games',\n    'AB': 'AtBats',\n    'H': 'Hits',\n    'BB': 'Walks',\n    'SO': 'Strikeouts'\n})\nprint(\"Renamed columns:\")\nprint(list(df_renamed.columns))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# .apply() with lambda — transform data row by row\n# Let's classify players into power tiers based on their HR count\ndf['Power_Tier'] = df['HR'].apply(lambda hr: 'Elite' if hr >= 40 else ('Above Avg' if hr >= 30 else 'Average'))\n\nprint(\"Player power tiers:\")\ndf[['Player', 'HR', 'Power_Tier']]",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Handling missing data (NaN values)\n# In real datasets, you will often encounter missing values\n# Let's simulate some missing data to learn how to handle it\n\ndf_missing = df[['Player', 'Team', 'HR', 'AVG']].copy()\n\n# Introduce some NaN values (simulating incomplete records)\ndf_missing.loc[2, 'HR'] = np.nan    # Betts' HR is missing\ndf_missing.loc[7, 'AVG'] = np.nan   # Devers' AVG is missing\ndf_missing.loc[9, 'HR'] = np.nan    # Turner's HR is missing\n\nprint(\"DataFrame with missing values:\")\nprint(df_missing)\nprint()\n\n# Detect missing values with .isna()\nprint(\"Missing value map:\")\nprint(df_missing.isna())\nprint()\nprint(f\"Total missing values per column:\\n{df_missing.isna().sum()}\")\nprint()\n\n# Fill missing values with .fillna()\ndf_filled = df_missing.fillna({'HR': df_missing['HR'].mean(), 'AVG': df_missing['AVG'].mean()})\nprint(\"After filling NaN with column means:\")\nprint(df_filled)\nprint()\n\n# Alternatively, drop rows with any missing values using .dropna()\ndf_dropped = df_missing.dropna()\nprint(f\"After dropping rows with NaN: {len(df_dropped)} rows remain (was {len(df_missing)})\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## 6. Sorting & Grouping\n\nSorting and grouping are essential for answering questions like \"Who leads the league in home runs?\" and \"Which team has the most combined RBIs?\"",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Sorting by a single column\n# Who has the most home runs? Sort descending\nprint(\"=== HR Leaders (most to fewest) ===\")\ndf.sort_values('HR', ascending=False)[['Player', 'Team', 'HR', 'RBI']]",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Sorting by multiple columns\n# Sort by Team first, then by HR within each team (descending)\nprint(\"=== Sorted by Team, then HR (descending) ===\")\ndf.sort_values(['Team', 'HR'], ascending=[True, False])[['Player', 'Team', 'HR', 'RBI']]",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Grouping with .groupby() — aggregate stats by category\n# \"How many total HR did each team hit?\"\nprint(\"=== Total HR by Team ===\")\nteam_hr = df.groupby('Team')['HR'].sum()\nprint(team_hr)\nprint()\n\n# Which team had the most combined home runs?\nprint(f\"Team with most HR: {team_hr.idxmax()} ({team_hr.max()} HR)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# .agg() — apply multiple aggregation functions at once\n# \"For each team, show total HR, total RBI, and average batting average\"\nprint(\"=== Multiple aggregations by Team ===\")\nteam_stats = df.groupby('Team').agg({\n    'HR': 'sum',\n    'RBI': 'sum',\n    'AVG': 'mean',\n    'Player': 'count'  # count of players per team\n})\n\n# Rename the 'Player' column to something more descriptive\nteam_stats = team_stats.rename(columns={'Player': 'Num_Players'})\n\n# Round the AVG column\nteam_stats['AVG'] = team_stats['AVG'].round(3)\n\nprint(team_stats)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# .value_counts() — count occurrences of each unique value\n# \"How many players play each position?\"\nprint(\"=== Position counts ===\")\nprint(df['Position'].value_counts())\nprint()\n\n# \"How many players per team?\"\nprint(\"=== Team counts ===\")\nprint(df['Team'].value_counts())\nprint()\n\n# \"How many players in each Power Tier?\"\nprint(\"=== Power Tier distribution ===\")\nprint(df['Power_Tier'].value_counts())",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# .pivot_table() — create a spreadsheet-style summary table\n# \"Average HR by Position and Power Tier\"\nprint(\"=== Pivot Table: Average HR by Position ===\")\npivot = df.pivot_table(\n    values='HR',\n    index='Position',\n    aggfunc=['mean', 'count']\n)\nprint(pivot)\nprint()\n\n# A more detailed pivot table\nprint(\"=== Pivot Table: Average stats by Team ===\")\npivot2 = df.pivot_table(\n    values=['HR', 'RBI', 'AVG'],\n    index='Team',\n    aggfunc='mean'\n).round(1)\nprint(pivot2)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## 7. Combining DataFrames\n\nIn real-world data analysis, your data is almost never in a single table. You will need to **merge** (join) tables that share a common column, or **concatenate** (stack) tables that have the same structure.\n\nThis is just like SQL JOINs and UNION operations — but done in Python with pandas.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# First, let's create a second DataFrame with different information about some of our players\n# This simulates having salary/biographical data in a separate table\n\nplayer_info = pd.DataFrame({\n    'Player': ['Aaron Judge', 'Shohei Ohtani', 'Mookie Betts', 'Juan Soto', 'Freddie Freeman',\n               'Corey Seager', 'Rafael Devers', 'Mike Trout'],  # Note: Trout is NOT in our batting df\n    'Age': [31, 29, 31, 25, 34, 29, 27, 31],\n    'Salary_M': [40.0, 70.0, 30.5, 31.0, 27.0, 32.5, 31.4, 37.1],  # Annual salary in millions\n    'Bats': ['R', 'L', 'R', 'L', 'L', 'L', 'L', 'R']\n})\n\nprint(\"=== Player Info Table ===\")\nprint(player_info)\nprint()\nprint(f\"Shape: {player_info.shape}\")\nprint(f\"Note: This table has {len(player_info)} players, including Mike Trout who is NOT in our batting stats table.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# pd.merge() — INNER JOIN (default)\n# Only keeps players that exist in BOTH tables\n# This is like SQL: SELECT * FROM df INNER JOIN player_info ON df.Player = player_info.Player\n\nmerged_inner = pd.merge(df[['Player', 'Team', 'HR', 'RBI', 'AVG']], player_info, on='Player')\n\nprint(\"=== Inner Merge (only players in BOTH tables) ===\")\nprint(f\"Batting stats table: {len(df)} players\")\nprint(f\"Player info table:   {len(player_info)} players\")\nprint(f\"Inner merge result:  {len(merged_inner)} players\")\nprint()\nmerged_inner",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Different join types — left, right, outer\n# Let's compare them to understand the differences\n\nbatting_subset = df[['Player', 'Team', 'HR']].copy()\n\n# LEFT JOIN — keep ALL rows from left table, match what you can from right\nmerged_left = pd.merge(batting_subset, player_info, on='Player', how='left')\n\n# RIGHT JOIN — keep ALL rows from right table, match what you can from left\nmerged_right = pd.merge(batting_subset, player_info, on='Player', how='right')\n\n# OUTER JOIN — keep ALL rows from BOTH tables\nmerged_outer = pd.merge(batting_subset, player_info, on='Player', how='outer')\n\nprint(f\"Left table (batting):  {len(batting_subset)} players\")\nprint(f\"Right table (info):    {len(player_info)} players\")\nprint()\nprint(f\"Inner join result:     {len(merged_inner)} players  (only matches)\")\nprint(f\"Left join result:      {len(merged_left)} players  (all from batting + matched info)\")\nprint(f\"Right join result:     {len(merged_right)} players  (all from info + matched batting)\")\nprint(f\"Outer join result:     {len(merged_outer)} players  (everyone from both)\")\nprint()\n\n# Show the outer join to see NaN values for unmatched rows\nprint(\"=== Outer Join (notice NaN for unmatched rows) ===\")\nmerged_outer",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# pd.concat() — stack DataFrames vertically (like SQL UNION)\n# Useful when you have data split across multiple tables with the same columns\n\n# Simulate having batting data split into two separate tables (e.g., AL and NL players)\nal_players = df[df['Team'].isin(['NYY', 'BOS', 'TOR'])][['Player', 'Team', 'HR', 'AVG']]\nnl_players = df[df['Team'].isin(['LAD', 'PHI'])][['Player', 'Team', 'HR', 'AVG']]\n\nprint(\"=== AL Players ===\")\nprint(al_players)\nprint()\nprint(\"=== NL Players ===\")\nprint(nl_players)\nprint()\n\n# Concatenate them back together\nall_players = pd.concat([al_players, nl_players], ignore_index=True)\nprint(f\"=== Combined ({len(al_players)} AL + {len(nl_players)} NL = {len(all_players)} total) ===\")\nall_players",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## 8. Wrap-Up\n\nLet's save our work and review what we learned.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Save our DataFrame to a CSV file for use in future notebooks\n# index=False prevents pandas from writing the row numbers as a column\nimport os\n\n# Create the data directory if it doesn't exist\nos.makedirs('data', exist_ok=True)\n\ndf.to_csv('data/sample_batting_stats.csv', index=False)\nprint(\"DataFrame saved to 'data/sample_batting_stats.csv'\")\nprint()\n\n# Verify by reading it back\ndf_loaded = pd.read_csv('data/sample_batting_stats.csv')\nprint(f\"Verified: loaded {df_loaded.shape[0]} rows and {df_loaded.shape[1]} columns from CSV\")\nprint(df_loaded.head(3))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Key Takeaways — Pandas Cheat Sheet\n\n| Task | Code |\n|------|------|\n| **Import** | `import pandas as pd` |\n| **Create DataFrame** | `df = pd.DataFrame(dict)` |\n| **Load CSV** | `df = pd.read_csv('file.csv')` |\n| **Save CSV** | `df.to_csv('file.csv', index=False)` |\n| **Explore** | `.head()`, `.tail()`, `.shape`, `.info()`, `.describe()` |\n| **Select columns** | `df['col']` or `df[['col1', 'col2']]` |\n| **Filter rows** | `df[df['col'] > value]` |\n| **Multiple filters** | `df[(cond1) & (cond2)]` or `df.query('...')` |\n| **Select by position** | `df.iloc[row, col]` |\n| **Select by label** | `df.loc[row, col]` |\n| **Add column** | `df['new'] = df['a'] + df['b']` |\n| **Rename columns** | `df.rename(columns={'old': 'new'})` |\n| **Apply function** | `df['col'].apply(lambda x: ...)` |\n| **Handle NaN** | `.isna()`, `.fillna()`, `.dropna()` |\n| **Sort** | `df.sort_values('col', ascending=False)` |\n| **Group & aggregate** | `df.groupby('col').agg({...})` |\n| **Count values** | `df['col'].value_counts()` |\n| **Pivot table** | `df.pivot_table(values, index, aggfunc)` |\n| **Merge (join)** | `pd.merge(df1, df2, on='col', how='inner')` |\n| **Concatenate (stack)** | `pd.concat([df1, df2], ignore_index=True)` |",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Next Up: Data Visualization with Matplotlib & Seaborn\n\nNow that you can load, clean, filter, and transform data with pandas, the next step is to **visualize** it. In **`03_data_visualization.ipynb`**, you will learn how to create:\n\n- Bar charts, scatter plots, and histograms\n- Multi-panel figures\n- Baseball-specific visualizations (spray charts, stat comparisons)\n- Publication-quality plots with custom styling\n\nSee you in the next notebook!",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}